{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60310997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c7de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import sentence_transformers\n",
    "import sentence_transformers.cross_encoder.evaluation\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, InputExample # High-level sentence encoders.\n",
    "import sentence_transformers.models as models\n",
    "import sentence_transformers.losses as losses\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm # Enables progress bars\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "QUICK_RUN = False # Config setting to switch between foreground (subset) and background (full-dataset) running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8cbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_dataset(\"BeIR/scidocs\", \"queries\", split=\"queries\")\n",
    "docs = load_dataset(\"BeIR/scidocs\", \"corpus\", split=\"corpus\")\n",
    "qrels = load_dataset(\"BeIR/scidocs-qrels\", delimiter=\"\\t\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7f48e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "25657\n",
      "29928\n",
      "1000\n",
      "25657\n"
     ]
    }
   ],
   "source": [
    "print(len(queries))\n",
    "print(len(docs))\n",
    "print(len(qrels))\n",
    "print(len(set(qrels[\"query-id\"])))\n",
    "print(len(set(qrels[\"corpus-id\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a91605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['_id', 'title', 'text'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['_id', 'title', 'text'],\n",
       "     num_rows: 25657\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['query-id', 'corpus-id', 'score'],\n",
       "     num_rows: 29928\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, docs, qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e90371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes only\n",
    "if QUICK_RUN:\n",
    "    queries = queries.select(range(100))\n",
    "    docs = docs.select(range(2500))\n",
    "    qrels = qrels.filter(lambda x: x[\"query-id\"] in queries[\"_id\"] and x[\"corpus-id\"] in docs[\"_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af14bf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 26935\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1497\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['query-id', 'corpus-id', 'score'],\n",
       "        num_rows: 1496\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 90% train, 10% test + validation\n",
    "train_testvalid = qrels.train_test_split(test_size=0.1, seed=1)\n",
    "\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=1)\n",
    "\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']  # Using the training split from the test_valid split as validation\n",
    "})\n",
    "\n",
    "train_test_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603cf20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Provable data possession at untrusted stores',\n",
       "  'StreamOp: An Innovative Middleware for Supporting Data Management and Query Functionalities over Sensor Network Streams Efficiently',\n",
       "  0),\n",
       " ('Rumor Detection and Classification for Twitter Data',\n",
       "  'Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews',\n",
       "  1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_triple_for_example(example):\n",
    "    q = queries[queries[\"_id\"].index(example[\"query-id\"])][\"text\"]\n",
    "    d = docs[docs[\"_id\"].index(example[\"corpus-id\"])][\"title\"]\n",
    "    r = example[\"score\"]\n",
    "    return q, d, r\n",
    "\n",
    "ex0 = get_triple_for_example(train_test_valid_dataset[\"test\"][0])\n",
    "ex1 = get_triple_for_example(train_test_valid_dataset[\"test\"][1])\n",
    "ex0, ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de6b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_distribution': {'labels': [1, 0], 'fractions': [0.16461852608130684, 0.8353814739186931]}, 'label_skew': 1.8087864265977875}\n",
      "{'label_distribution': {'labels': [0, 1], 'fractions': [0.8348930481283422, 0.16510695187165775]}, 'label_skew': 1.8040061996868444}\n",
      "{'label_distribution': {'labels': [0, 1], 'fractions': [0.8350033400133601, 0.16499665998663995]}, 'label_skew': 1.8050841379113802}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from scipy import stats\n",
    "\n",
    "# From Huggingface Evaluate\n",
    "def label_dist(data):\n",
    "    \"\"\"Returns the fraction of each label present in the data\"\"\"\n",
    "    c = Counter(data)\n",
    "    label_distribution = {\"labels\": [k for k in c.keys()], \"fractions\": [f / len(data) for f in c.values()]}\n",
    "    \n",
    "    if isinstance(data[0], str):\n",
    "        label2id = {label: id for id, label in enumerate(label_distribution[\"labels\"])}\n",
    "        data = [label2id[d] for d in data]\n",
    "    \n",
    "    skew = stats.skew(data)\n",
    "    \n",
    "    return {\"label_distribution\": label_distribution, \"label_skew\": skew}\n",
    "\n",
    "print(label_dist(data=train_test_valid_dataset[\"train\"][\"score\"]))\n",
    "print(label_dist(data=train_test_valid_dataset[\"valid\"][\"score\"]))\n",
    "print(label_dist(data=train_test_valid_dataset[\"test\"][\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e377c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86715911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A hybrid of genetic algorithm and particle swarm optimization for recurrent network design: An evolutionary recurrent network which automates the design of recurrent neural/fuzzy networks using a new evolutionary learning algorithm is proposed in this paper. This new evolutionary learning algorithm is based on a hybrid of genetic algorithm (GA) and particle swarm optimization (PSO), and is thus called HGAPSO. In HGAPSO, individuals in a new generation are created, not only by crossover and mutation operation as in GA, but also by PSO. The concept of elite strategy is adopted in HGAPSO, where the upper-half of the best-performing individuals in a population are regarded as elites. However, instead of being reproduced directly to the next generation, these elites are first enhanced. The group constituted by the elites is regarded as a swarm, and each elite corresponds to a particle within it. In this regard, the elites are enhanced by PSO, an operation which mimics the maturing phenomenon in nature. These enhanced elites constitute half of the population in the new generation, whereas the other half is generated by performing crossover and mutation operation on these enhanced elites. HGAPSO is applied to recurrent neural/fuzzy network design as follows. For recurrent neural network, a fully connected recurrent neural network is designed and applied to a temporal sequence production problem. For recurrent fuzzy network design, a Takagi-Sugeno-Kang-type recurrent fuzzy network is designed and applied to dynamic plant control. The performance of HGAPSO is compared to both GA and PSO in these recurrent networks design problems, demonstrating its superiority.',\n",
       " 'A Hybrid EP and SQP for Dynamic Economic Dispatch with Nonsmooth Fuel Cost Function: Dynamic economic dispatch (DED) is one of the main functions of power generation operation and control. It determines the optimal settings of generator units with predicted load demand over a certain period of time. The objective is to operate an electric power system most economically while the system is operating within its security limits. This paper proposes a new hybrid methodology for solving DED. The proposed method is developed in such a way that a simple evolutionary programming (EP) is applied as a based level search, which can give a good direction to the optimal global region, and a local search sequential quadratic programming (SQP) is used as a fine tuning to determine the optimal solution at the final. Ten units test system with nonsmooth fuel cost function is used to illustrate the effectiveness of the proposed method compared with those obtained from EP and SQP alone.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.map(lambda x: {\"title_text\": x[\"title\"] + \": \" + x[\"text\"]})[\"title_text\"][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ec1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10d9212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f49db7-2ccd-42a9-a9f3-1bde88e4b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_for_analysis = docs.map(lambda x: {\"title_text\": x[\"title\"] + \": \" + x[\"text\"]})[\"title_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74fc8d7d-50af-4fff-8f60-794f0a7f9f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>8835</td>\n",
       "      <td>-1_object_scene_objects_cognitive</td>\n",
       "      <td>[object, scene, objects, cognitive, health, vi...</td>\n",
       "      <td>[Is Verbal Irony Special?: The way we speak ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>0_antenna_patch_radiation_polarized</td>\n",
       "      <td>[antenna, patch, radiation, polarized, microst...</td>\n",
       "      <td>[Low-Cost High-Gain and Broadband Substrate- I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>318</td>\n",
       "      <td>1_encryption_encrypted_cipher_cryptographic</td>\n",
       "      <td>[encryption, encrypted, cipher, cryptographic,...</td>\n",
       "      <td>[Ring-LWE Ciphertext Compression and Error Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>231</td>\n",
       "      <td>2_reinforcement_rl_policy_reward</td>\n",
       "      <td>[reinforcement, rl, policy, reward, agent, cri...</td>\n",
       "      <td>[Deep Reinforcement Learning framework for Aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>215</td>\n",
       "      <td>3_sentiment_opinion_polarity_opinions</td>\n",
       "      <td>[sentiment, opinion, polarity, opinions, sarca...</td>\n",
       "      <td>[Domain Specific Sentence Level Mood Extractio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                         Name  \\\n",
       "0     -1   8835            -1_object_scene_objects_cognitive   \n",
       "1      0    330          0_antenna_patch_radiation_polarized   \n",
       "2      1    318  1_encryption_encrypted_cipher_cryptographic   \n",
       "3      2    231             2_reinforcement_rl_policy_reward   \n",
       "4      3    215        3_sentiment_opinion_polarity_opinions   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [object, scene, objects, cognitive, health, vi...   \n",
       "1  [antenna, patch, radiation, polarized, microst...   \n",
       "2  [encryption, encrypted, cipher, cryptographic,...   \n",
       "3  [reinforcement, rl, policy, reward, agent, cri...   \n",
       "4  [sentiment, opinion, polarity, opinions, sarca...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Is Verbal Irony Special?: The way we speak ca...  \n",
       "1  [Low-Cost High-Gain and Broadband Substrate- I...  \n",
       "2  [Ring-LWE Ciphertext Compression and Error Cor...  \n",
       "3  [Deep Reinforcement Learning framework for Aut...  \n",
       "4  [Domain Specific Sentence Level Mood Extractio...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(embedding_model=model_name, ctfidf_model=ClassTfidfTransformer(reduce_frequent_words=True))\n",
    "topic_model.fit(docs_for_analysis)\n",
    "\n",
    "topic_model.get_topic_info().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceefc85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bertopic_doc_embeddings.html'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.reduce_topics(docs_for_analysis, nr_topics=15)\n",
    "fig = topic_model.visualize_documents(docs_for_analysis)\n",
    "plotly.offline.plot(fig, filename='bertopic_doc_embeddings.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de8bbf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"bertopic_doc_embeddings.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd56531e5f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='bertopic_doc_embeddings.html', width=1200, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db78e3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cce8fe5e99a48c1bf27e7f0ea7a20a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416acca745e1484298b772325bab480d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb4134f5880433584832c5ca0f878c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d24027a27c74ec989bcc520537e2a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c0c7034613408098548695d005f34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69446324963d4a158f747bebbe06a73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized text:\n",
      "['this', 'is', 'the', 'first', 'sentence', 'with', 'complex', 'token', '##s', ',', 'such', 'as', 'sentence', '##tra', '##ns', '##form', '##ers', '.', 'we', 'can', 'batch', 'multiple', 'sentences', '.']\n",
      "\n",
      "Token IDs:\n",
      "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034,  6251,  2007,  3375, 19204,  2015,\n",
      "          1010,  2107,  2004,  6251,  6494,  3619, 14192,  2545,  1012,   102],\n",
      "        [  101,  2057,  2064, 14108,  3674, 11746,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "\n",
      "Output Dictionary:\n",
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "\n",
      "Output Size:\n",
      "torch.Size([2, 20, 384])\n",
      "\n",
      "Contextualized Token Embeddings (truncated):\n",
      "tensor([[[-0.1560, -0.1154,  0.0731,  0.2994,  0.0485,  0.0800,  0.1049],\n",
      "         [ 0.2525,  0.9195,  0.4429,  0.7421,  0.4402,  0.2827,  1.2313],\n",
      "         [ 0.0174,  0.3108,  0.0699,  0.3103,  0.2146,  0.4123,  0.3589]],\n",
      "\n",
      "        [[ 0.1666, -0.2453,  0.0160,  0.0463,  0.0391,  0.0391,  0.2853],\n",
      "         [ 0.1173, -0.0858,  0.0132, -0.0607,  0.2865,  0.2517,  0.5110],\n",
      "         [-0.0912, -0.3036, -0.3637,  0.1121,  0.0017,  0.5838,  0.1054]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Pooled Embeddings (truncated):\n",
      "torch.Size([2, 384]) tensor([[-0.0595,  0.0151,  0.0587,  0.0922, -0.0913, -0.0640,  0.0419],\n",
      "        [ 0.0048,  0.0145,  0.0123,  0.0590, -0.1448, -0.0045, -0.0301]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "Predicted Values (not fine-tuning)\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0094, -0.0092],\n",
      "        [ 0.0116,  0.0185]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# Tokenizer and model must match\n",
    "ex_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ex_model = AutoModel.from_pretrained(model_name)\n",
    "ex_model_with_head = AutoModelForSequenceClassification.from_pretrained(model_name)  # Needs fine-tuning, here for demonstration\n",
    "\n",
    "test_sentences = [\"This is the first sentence with complex tokens, such as SentenceTransformers.\", \"We can batch multiple sentences.\"]\n",
    "ex_tokenized = ex_tokenizer(test_sentences, return_tensors=\"pt\", padding=True, truncation=True)  # Collates data with padding\n",
    "ex_res = ex_model(**ex_tokenized)\n",
    "ex_res_with_head = ex_model_with_head(**ex_tokenized)\n",
    "\n",
    "print(\"\\nTokenized text:\")  # Word Piece Tokenization\n",
    "print(ex_tokenizer.tokenize(test_sentences))\n",
    "\n",
    "print(\"\\nToken IDs:\")\n",
    "print(ex_tokenized)\n",
    "\n",
    "print(\"\\nOutput Dictionary:\")\n",
    "print(ex_res.keys())\n",
    "\n",
    "print(\"\\nOutput Size:\")\n",
    "print(ex_res.last_hidden_state.size())\n",
    "\n",
    "print(\"\\nContextualized Token Embeddings (truncated):\")\n",
    "print(ex_res.last_hidden_state[:, :3, :7])  # First 3 tokens\n",
    "\n",
    "print(\"\\nPooled Embeddings (truncated):\")\n",
    "print(ex_res.pooler_output.shape, ex_res.pooler_output[:, :7])\n",
    "\n",
    "print(\"\\nPredicted Values (not fine-tuning)\")\n",
    "print(ex_res_with_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "899af399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses Mean pooling\n",
    "topic_model.embedding_model.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c62297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starts with embeddings\n",
    "topic_model.embedding_model.embedding_model[0]._modules[\"auto_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adf3f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import InputExample\n",
    " \n",
    "class IRDataset(Dataset):\n",
    "    def __init__(self, queries_ds, docs_ds, qrel_ds, mode=\"cross\"):\n",
    "        self.mode = mode\n",
    "        qrels = defaultdict(set)\n",
    " \n",
    "        def transform(x):\n",
    "            q, d, r = x[\"query-id\"], x[\"corpus-id\"], x[\"score\"]\n",
    "            q_idx = queries_ds[\"_id\"].index(q)\n",
    "            x[\"query_text\"] = queries_ds[q_idx][\"text\"]\n",
    "            d_idx = docs_ds[\"_id\"].index(d)\n",
    "            x[\"doc_content\"] = docs_ds[d_idx][\"title\"] + \": \" + docs_ds[d_idx][\"text\"]\n",
    "            x[\"label\"] = float(r)\n",
    "            if r:\n",
    "                qrels[q].add(d)\n",
    "            return x\n",
    " \n",
    "        qrel_ds = qrel_ds.map(transform)\n",
    "        self.q_ids = qrel_ds[\"query-id\"]\n",
    "        self.d_ids = qrel_ds[\"corpus-id\"]\n",
    "        self.qrels = qrels\n",
    "        self.queries = qrel_ds[\"query_text\"]\n",
    "        self.docs = qrel_ds[\"doc_content\"]\n",
    "        self.labels = qrel_ds[\"label\"]\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        qs = self.queries[idx]\n",
    "        ds = self.docs[idx]\n",
    "        guid = f\"{self.q_ids[idx]}_{self.d_ids[idx]}\"  # Generating a unique identifier\n",
    "    \n",
    "        if self.mode == \"rep\":\n",
    "            if type(idx) is int:\n",
    "                text_list = [{\"query\": qs}, {\"doc\": ds}]\n",
    "            else:\n",
    "                text_list = [[{\"query\": q} for q in qs], [{\"doc\": d} for d in ds]]\n",
    "            return InputExample(guid=guid, text_a=qs, text_b=ds, label=self.labels[idx])\n",
    "        return InputExample(guid=guid, text_a=qs, text_b=ds, label=self.labels[idx])\n",
    " \n",
    "    def set_mode(self, mode):\n",
    "        self.mode = mode\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45e3242c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'eecbbb0ba7b513a2fe1e7a0131213e5a94b1868a_fb0031e4d2a7358fca04da94c5d7e52da794fe34',\n",
       " 'text_a': 'Toward an IT governance maturity self-assessment model using EFQM and CobiT',\n",
       " 'text_b': 'A maturity model for information governance: Information Governance (IG) as defined by Gartner is the “specification of decision rights and an accountability framework to encourage desirable behavior in the valuation, creation, storage, use, archival and deletion of information. Includes the processes, roles, standards and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals”. In this paper, we present how to create an IG maturity model based on existing reference documents. The process is based on existing maturity model development methods. These methods allow for a systematic approach to maturity model development backed up by a well-known and proved scientific research method called Design Science Research. Then, based on the maturity model proposed in this paper, an assessment is conducted and the results are presented, this assessment was conducted as a self-assessment in the context of the EC-funded E-ARK project for the seven pilots of the project. The main conclusion from this initial assessment is that there is much room for improvement with most pilots achieving results between maturity level two and three. As future work, the goal is to analyze other references from different domains, such as, records management. These references will enhance, detail and help develop the maturity model making it even more valuable for all types of organization that deal with information governance.',\n",
       " 'label': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = IRDataset(queries, docs, train_test_valid_dataset[\"train\"])\n",
    "valid_ds = IRDataset(queries, docs, train_test_valid_dataset[\"valid\"])\n",
    "train_ds[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d243a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "monoBERT = CrossEncoder(model_name, # We use cross-encoder as monoBERT example\n",
    "                        num_labels=1, # Perform binary classification\n",
    "                        device=None, # Will use CUDA if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b4ec3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49718797, 0.48692062], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monoBERT.predict([ex0[:2], ex1[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1d0e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputExample(guid='eecbbb0ba7b513a2fe1e7a0131213e5a94b1868a_fb0031e4d2a7358fca04da94c5d7e52da794fe34', text_a='Toward an IT governance maturity self-assessment model using EFQM and CobiT', text_b='A maturity model for information governance: Information Governance (IG) as defined by Gartner is the “specification of decision rights and an accountability framework to encourage desirable behavior in the valuation, creation, storage, use, archival and deletion of information. Includes the processes, roles, standards and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals”. In this paper, we present how to create an IG maturity model based on existing reference documents. The process is based on existing maturity model development methods. These methods allow for a systematic approach to maturity model development backed up by a well-known and proved scientific research method called Design Science Research. Then, based on the maturity model proposed in this paper, an assessment is conducted and the results are presented, this assessment was conducted as a self-assessment in the context of the EC-funded E-ARK project for the seven pilots of the project. The main conclusion from this initial assessment is that there is much room for improvement with most pilots achieving results between maturity level two and three. As future work, the goal is to analyze other references from different domains, such as, records management. These references will enhance, detail and help develop the maturity model making it even more valuable for all types of organization that deal with information governance.', label=1.0)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e0c18dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "842"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=32)\n",
    "# We need sentence pairs format for the library here.\n",
    "# valid_dl = DataLoader(valid_ds, batch_size=32)\n",
    "sentence_pairs = list(zip(valid_ds.queries, valid_ds.docs))\n",
    "labels = valid_ds.labels\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37db30c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['config', 'model', 'tokenizer', 'max_length', '_target_device', 'default_activation_function'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monoBERT.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e352be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "repBased = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da4768d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1819, 0.1325],\n",
       "        [0.0353, 1.0000]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs, ds = repBased.encode([{\"query\": ex0[0]}, {\"query\": ex1[0]}]), repBased.encode([{\"doc\": ex0[1]}, {\"doc\": ex1[0]}])\n",
    "sentence_transformers.util.cos_sim(qs, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c396f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_mode(\"rep\")\n",
    "valid_ds.set_mode(\"rep\")\n",
    "train_dl_repBased = DataLoader(train_ds, batch_size=32, collate_fn=repBased.smart_batching_collate)\n",
    "valid_dl_repBased = DataLoader(valid_ds, batch_size=32, collate_fn=repBased.smart_batching_collate)\n",
    "# assert next(iter(train_dl_repBased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e0bc2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_dict = dict(zip(valid_ds.q_ids, valid_ds.queries))\n",
    "docs_dict = dict(zip(valid_ds.d_ids, valid_ds.docs))\n",
    "qrels_dict = valid_ds.qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "376d56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_evaluator = sentence_transformers.evaluation.InformationRetrievalEvaluator(queries_dict, docs_dict, qrels_dict, write_csv=True)\n",
    "\n",
    "repBased.fit(\n",
    "    train_objectives=[(train_dl_repBased, losses.CosineSimilarityLoss(repBased))],\n",
    "    evaluator=ir_evaluator,\n",
    "    epochs=10,\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    show_progress_bar=True,\n",
    "    save_best_model=True,\n",
    "    output_path=\"./\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3846bbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1819, 0.1325],\n",
       "        [0.0353, 1.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs, ds = repBased.encode([{\"query\": ex0[0]}, {\"query\": ex1[0]}]), repBased.encode([{\"doc\": ex0[1]}, {\"doc\": ex1[0]}])\n",
    "sentence_transformers.util.cos_sim(qs, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d19e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"eval/Information-Retrieval_evaluation_results.csv\")\n",
    "df.tail(n=10)\n",
    "\n",
    "df.set_index(\"epoch\").drop(columns=[\"steps\"]).plot(legend=False)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb5c96-43f2-42cf-a20a-b4b28e9100c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
